{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bno-wuuaqwpX",
   "metadata": {
    "id": "bno-wuuaqwpX"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.linear_model import SGDOneClassSVM\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78633f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>frame_time</th>\n",
       "      <th>total_pacotes</th>\n",
       "      <th>total_pacotes_icmp</th>\n",
       "      <th>total_pacotes_udp</th>\n",
       "      <th>total_pacotes_tcp</th>\n",
       "      <th>maior_pacote</th>\n",
       "      <th>menor_pacote</th>\n",
       "      <th>soma_pacotes</th>\n",
       "      <th>total_ips_origem</th>\n",
       "      <th>...</th>\n",
       "      <th>10_coefficient_variation_std_tcp_time_relative</th>\n",
       "      <th>10_lag-1AC_std_tcp_time_relative</th>\n",
       "      <th>10_lag-2AC_std_tcp_time_relative</th>\n",
       "      <th>10_lag-3AC_std_tcp_time_relative</th>\n",
       "      <th>10_skw_mean_tcp_time_relative</th>\n",
       "      <th>10_kurt_mean_tcp_time_relative</th>\n",
       "      <th>10_coefficient_variation_mean_tcp_time_relative</th>\n",
       "      <th>10_lag-1AC_mean_tcp_time_relative</th>\n",
       "      <th>10_lag-2AC_mean_tcp_time_relative</th>\n",
       "      <th>10_lag-3AC_mean_tcp_time_relative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Aug 18, 2011 08:19:13.328377000 UTC</td>\n",
       "      <td>1343</td>\n",
       "      <td>1</td>\n",
       "      <td>597</td>\n",
       "      <td>710</td>\n",
       "      <td>1514</td>\n",
       "      <td>60</td>\n",
       "      <td>1063048</td>\n",
       "      <td>108</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Aug 18, 2011 08:19:14.001200000 UTC</td>\n",
       "      <td>1829</td>\n",
       "      <td>4</td>\n",
       "      <td>764</td>\n",
       "      <td>978</td>\n",
       "      <td>1514</td>\n",
       "      <td>60</td>\n",
       "      <td>1352117</td>\n",
       "      <td>145</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Aug 18, 2011 08:19:15.005031000 UTC</td>\n",
       "      <td>1794</td>\n",
       "      <td>4</td>\n",
       "      <td>859</td>\n",
       "      <td>888</td>\n",
       "      <td>1514</td>\n",
       "      <td>60</td>\n",
       "      <td>1274290</td>\n",
       "      <td>183</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Aug 18, 2011 08:19:16.004984000 UTC</td>\n",
       "      <td>1797</td>\n",
       "      <td>3</td>\n",
       "      <td>643</td>\n",
       "      <td>1110</td>\n",
       "      <td>1514</td>\n",
       "      <td>60</td>\n",
       "      <td>1244046</td>\n",
       "      <td>159</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Aug 18, 2011 08:19:17.000069000 UTC</td>\n",
       "      <td>2047</td>\n",
       "      <td>5</td>\n",
       "      <td>889</td>\n",
       "      <td>1104</td>\n",
       "      <td>1514</td>\n",
       "      <td>60</td>\n",
       "      <td>1415830</td>\n",
       "      <td>176</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8798</th>\n",
       "      <td>8798</td>\n",
       "      <td>Aug 18, 2011 10:45:51.000139000 UTC</td>\n",
       "      <td>13063</td>\n",
       "      <td>0</td>\n",
       "      <td>271</td>\n",
       "      <td>12729</td>\n",
       "      <td>1514</td>\n",
       "      <td>60</td>\n",
       "      <td>12434675</td>\n",
       "      <td>193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198314</td>\n",
       "      <td>0.373041</td>\n",
       "      <td>0.188673</td>\n",
       "      <td>0.228270</td>\n",
       "      <td>-0.778374</td>\n",
       "      <td>1.919347</td>\n",
       "      <td>0.295774</td>\n",
       "      <td>0.547461</td>\n",
       "      <td>0.455502</td>\n",
       "      <td>0.467108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8799</th>\n",
       "      <td>8799</td>\n",
       "      <td>Aug 18, 2011 10:45:52.001829000 UTC</td>\n",
       "      <td>11448</td>\n",
       "      <td>0</td>\n",
       "      <td>223</td>\n",
       "      <td>11160</td>\n",
       "      <td>1514</td>\n",
       "      <td>60</td>\n",
       "      <td>10711927</td>\n",
       "      <td>149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198197</td>\n",
       "      <td>0.372345</td>\n",
       "      <td>0.187880</td>\n",
       "      <td>0.227413</td>\n",
       "      <td>-0.779524</td>\n",
       "      <td>1.925385</td>\n",
       "      <td>0.295608</td>\n",
       "      <td>0.546494</td>\n",
       "      <td>0.455061</td>\n",
       "      <td>0.466422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8800</th>\n",
       "      <td>8800</td>\n",
       "      <td>Aug 18, 2011 10:45:53.000249000 UTC</td>\n",
       "      <td>19537</td>\n",
       "      <td>0</td>\n",
       "      <td>332</td>\n",
       "      <td>19158</td>\n",
       "      <td>1514</td>\n",
       "      <td>60</td>\n",
       "      <td>18804518</td>\n",
       "      <td>194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197878</td>\n",
       "      <td>0.370958</td>\n",
       "      <td>0.186528</td>\n",
       "      <td>0.227835</td>\n",
       "      <td>-0.772473</td>\n",
       "      <td>1.925770</td>\n",
       "      <td>0.295801</td>\n",
       "      <td>0.547163</td>\n",
       "      <td>0.453754</td>\n",
       "      <td>0.467004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8801</th>\n",
       "      <td>8801</td>\n",
       "      <td>Aug 18, 2011 10:45:54.000253000 UTC</td>\n",
       "      <td>23437</td>\n",
       "      <td>0</td>\n",
       "      <td>234</td>\n",
       "      <td>23186</td>\n",
       "      <td>1514</td>\n",
       "      <td>60</td>\n",
       "      <td>22579857</td>\n",
       "      <td>139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197621</td>\n",
       "      <td>0.370033</td>\n",
       "      <td>0.186868</td>\n",
       "      <td>0.225963</td>\n",
       "      <td>-0.772488</td>\n",
       "      <td>1.939248</td>\n",
       "      <td>0.295409</td>\n",
       "      <td>0.547758</td>\n",
       "      <td>0.454853</td>\n",
       "      <td>0.465247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8802</th>\n",
       "      <td>8802</td>\n",
       "      <td>Aug 18, 2011 10:45:55.000156000 UTC</td>\n",
       "      <td>8556</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>8430</td>\n",
       "      <td>1514</td>\n",
       "      <td>60</td>\n",
       "      <td>8313828</td>\n",
       "      <td>89</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198786</td>\n",
       "      <td>0.367613</td>\n",
       "      <td>0.183959</td>\n",
       "      <td>0.221747</td>\n",
       "      <td>-0.772755</td>\n",
       "      <td>1.933245</td>\n",
       "      <td>0.295589</td>\n",
       "      <td>0.542083</td>\n",
       "      <td>0.447020</td>\n",
       "      <td>0.458861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8803 rows Ã— 667 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                           frame_time  total_pacotes  \\\n",
       "0              0  Aug 18, 2011 08:19:13.328377000 UTC           1343   \n",
       "1              1  Aug 18, 2011 08:19:14.001200000 UTC           1829   \n",
       "2              2  Aug 18, 2011 08:19:15.005031000 UTC           1794   \n",
       "3              3  Aug 18, 2011 08:19:16.004984000 UTC           1797   \n",
       "4              4  Aug 18, 2011 08:19:17.000069000 UTC           2047   \n",
       "...          ...                                  ...            ...   \n",
       "8798        8798  Aug 18, 2011 10:45:51.000139000 UTC          13063   \n",
       "8799        8799  Aug 18, 2011 10:45:52.001829000 UTC          11448   \n",
       "8800        8800  Aug 18, 2011 10:45:53.000249000 UTC          19537   \n",
       "8801        8801  Aug 18, 2011 10:45:54.000253000 UTC          23437   \n",
       "8802        8802  Aug 18, 2011 10:45:55.000156000 UTC           8556   \n",
       "\n",
       "      total_pacotes_icmp  total_pacotes_udp  total_pacotes_tcp  maior_pacote  \\\n",
       "0                      1                597                710          1514   \n",
       "1                      4                764                978          1514   \n",
       "2                      4                859                888          1514   \n",
       "3                      3                643               1110          1514   \n",
       "4                      5                889               1104          1514   \n",
       "...                  ...                ...                ...           ...   \n",
       "8798                   0                271              12729          1514   \n",
       "8799                   0                223              11160          1514   \n",
       "8800                   0                332              19158          1514   \n",
       "8801                   0                234              23186          1514   \n",
       "8802                   0                 89               8430          1514   \n",
       "\n",
       "      menor_pacote  soma_pacotes  total_ips_origem  ...  \\\n",
       "0               60       1063048               108  ...   \n",
       "1               60       1352117               145  ...   \n",
       "2               60       1274290               183  ...   \n",
       "3               60       1244046               159  ...   \n",
       "4               60       1415830               176  ...   \n",
       "...            ...           ...               ...  ...   \n",
       "8798            60      12434675               193  ...   \n",
       "8799            60      10711927               149  ...   \n",
       "8800            60      18804518               194  ...   \n",
       "8801            60      22579857               139  ...   \n",
       "8802            60       8313828                89  ...   \n",
       "\n",
       "      10_coefficient_variation_std_tcp_time_relative  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "...                                              ...   \n",
       "8798                                        0.198314   \n",
       "8799                                        0.198197   \n",
       "8800                                        0.197878   \n",
       "8801                                        0.197621   \n",
       "8802                                        0.198786   \n",
       "\n",
       "      10_lag-1AC_std_tcp_time_relative  10_lag-2AC_std_tcp_time_relative  \\\n",
       "0                                  NaN                               NaN   \n",
       "1                                  NaN                               NaN   \n",
       "2                                  NaN                               NaN   \n",
       "3                                  NaN                               NaN   \n",
       "4                                  NaN                               NaN   \n",
       "...                                ...                               ...   \n",
       "8798                          0.373041                          0.188673   \n",
       "8799                          0.372345                          0.187880   \n",
       "8800                          0.370958                          0.186528   \n",
       "8801                          0.370033                          0.186868   \n",
       "8802                          0.367613                          0.183959   \n",
       "\n",
       "      10_lag-3AC_std_tcp_time_relative  10_skw_mean_tcp_time_relative  \\\n",
       "0                                  NaN                            NaN   \n",
       "1                                  NaN                            NaN   \n",
       "2                                  NaN                            NaN   \n",
       "3                                  NaN                            NaN   \n",
       "4                                  NaN                            NaN   \n",
       "...                                ...                            ...   \n",
       "8798                          0.228270                      -0.778374   \n",
       "8799                          0.227413                      -0.779524   \n",
       "8800                          0.227835                      -0.772473   \n",
       "8801                          0.225963                      -0.772488   \n",
       "8802                          0.221747                      -0.772755   \n",
       "\n",
       "      10_kurt_mean_tcp_time_relative  \\\n",
       "0                                NaN   \n",
       "1                                NaN   \n",
       "2                                NaN   \n",
       "3                                NaN   \n",
       "4                                NaN   \n",
       "...                              ...   \n",
       "8798                        1.919347   \n",
       "8799                        1.925385   \n",
       "8800                        1.925770   \n",
       "8801                        1.939248   \n",
       "8802                        1.933245   \n",
       "\n",
       "      10_coefficient_variation_mean_tcp_time_relative  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                                 NaN   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "...                                               ...   \n",
       "8798                                         0.295774   \n",
       "8799                                         0.295608   \n",
       "8800                                         0.295801   \n",
       "8801                                         0.295409   \n",
       "8802                                         0.295589   \n",
       "\n",
       "      10_lag-1AC_mean_tcp_time_relative  10_lag-2AC_mean_tcp_time_relative  \\\n",
       "0                                   NaN                                NaN   \n",
       "1                                   NaN                                NaN   \n",
       "2                                   NaN                                NaN   \n",
       "3                                   NaN                                NaN   \n",
       "4                                   NaN                                NaN   \n",
       "...                                 ...                                ...   \n",
       "8798                           0.547461                           0.455502   \n",
       "8799                           0.546494                           0.455061   \n",
       "8800                           0.547163                           0.453754   \n",
       "8801                           0.547758                           0.454853   \n",
       "8802                           0.542083                           0.447020   \n",
       "\n",
       "      10_lag-3AC_mean_tcp_time_relative  \n",
       "0                                   NaN  \n",
       "1                                   NaN  \n",
       "2                                   NaN  \n",
       "3                                   NaN  \n",
       "4                                   NaN  \n",
       "...                                 ...  \n",
       "8798                           0.467108  \n",
       "8799                           0.466422  \n",
       "8800                           0.467004  \n",
       "8801                           0.465247  \n",
       "8802                           0.458861  \n",
       "\n",
       "[8803 rows x 667 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_warning_url = \"early_warning_exp_1.csv\"\n",
    "early_warning = pd.read_csv(early_warning_url, sep=\";\")\n",
    "early_warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b473075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas = early_warning.columns[361:]\n",
    "df = early_warning[colunas]\n",
    "df = df.fillna(0)\n",
    "label = pd.read_csv('ctulabel.csv')['maior_2']\n",
    "\n",
    "init = 880\n",
    "train = 2934\n",
    "end = 5632\n",
    "\n",
    "x_train = df[init:train]\n",
    "x_test = df[train:end]\n",
    "y_real = label[train:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63cdf93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneClassSVM(gamma='auto', kernel='linear', nu=0.11)\n",
      "[[2605   78]\n",
      " [  13    2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9950    0.9709    0.9828      2683\n",
      "           1     0.0250    0.1333    0.0421        15\n",
      "\n",
      "    accuracy                         0.9663      2698\n",
      "   macro avg     0.5100    0.5521    0.5125      2698\n",
      "weighted avg     0.9896    0.9663    0.9776      2698\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fica = FastICA(n_components=9, random_state=0).fit(x_train)\n",
    "train_fica  = fica.transform(x_train)\n",
    "clf = OneClassSVM(gamma='auto', kernel='linear', nu=0.11).fit(train_fica)\n",
    "novo_test = []\n",
    "preds = []\n",
    "for i in range (0,90):\n",
    "    comeco = i*30\n",
    "    final = (i+1)*30\n",
    "    if(final > end):\n",
    "        final = end\n",
    "    y_test = clf.predict(fica.transform(x_test[comeco:final]))\n",
    "    y_test_final = [0 if i == 1 else 1 for i in y_test]\n",
    "    preds += y_test_final\n",
    "    \n",
    "print(clf)\n",
    "print(confusion_matrix(y_real, preds))\n",
    "print(classification_report(y_real, preds, digits = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5357f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneClassSVM(gamma='auto', kernel='linear', nu=0.47)\n",
      "[[2635   48]\n",
      " [  13    2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9951    0.9821    0.9886      2683\n",
      "           1     0.0400    0.1333    0.0615        15\n",
      "\n",
      "    accuracy                         0.9774      2698\n",
      "   macro avg     0.5175    0.5577    0.5250      2698\n",
      "weighted avg     0.9898    0.9774    0.9834      2698\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fa = FactorAnalysis(n_components=9, random_state=0).fit(x_train)\n",
    "train_fa  = fa.transform(x_train)\n",
    "clf = OneClassSVM(gamma='auto', kernel='linear', nu=0.47).fit(train_fa)\n",
    "novo_test = []\n",
    "preds = []\n",
    "for i in range (0,90):\n",
    "    comeco = i*30\n",
    "    final = (i+1)*30\n",
    "    if(final > end):\n",
    "        final = end\n",
    "    y_test = clf.predict(fa.transform(x_test[comeco:final]))\n",
    "    y_test_final = [0 if i == 1 else 1 for i in y_test]\n",
    "    preds += y_test_final\n",
    "    \n",
    "print(clf)\n",
    "print(confusion_matrix(y_real, preds))\n",
    "print(classification_report(y_real, preds, digits = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b794fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneClassSVM(gamma='auto', kernel='sigmoid', nu=0.22)\n",
      "[[2302  381]\n",
      " [  10    5]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9957    0.8580    0.9217      2683\n",
      "           1     0.0130    0.3333    0.0249        15\n",
      "\n",
      "    accuracy                         0.8551      2698\n",
      "   macro avg     0.5043    0.5957    0.4733      2698\n",
      "weighted avg     0.9902    0.8551    0.9167      2698\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=9, random_state=0).fit(x_train)\n",
    "train_pca  = pca.transform(x_train)\n",
    "clf = OneClassSVM(gamma='auto', kernel='sigmoid', nu=0.22).fit(train_pca)\n",
    "novo_test = []\n",
    "preds = []\n",
    "for i in range (0,90):\n",
    "    comeco = i*30\n",
    "    final = (i+1)*30\n",
    "    if(final > end):\n",
    "        final = end\n",
    "    y_test = clf.predict(pca.transform(x_test[comeco:final]))\n",
    "    y_test_final = [0 if i == 1 else 1 for i in y_test]\n",
    "    preds += y_test_final\n",
    "    \n",
    "print(clf)\n",
    "print(confusion_matrix(y_real, preds))\n",
    "print(classification_report(y_real, preds, digits = 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148c9e3c",
   "metadata": {},
   "source": [
    "# novo teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae9b491c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[4503  197]\n",
      " [  49    3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9892    0.9581    0.9734      4700\n",
      "           1     0.0150    0.0577    0.0238        52\n",
      "\n",
      "    accuracy                         0.9482      4752\n",
      "   macro avg     0.5021    0.5079    0.4986      4752\n",
      "weighted avg     0.9786    0.9482    0.9630      4752\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = '10'\n",
    "colunas = [file+'_kurt_total_ips_origem',\n",
    "           file+'_skw_total_ips_origem',\n",
    "           file+'_coefficient_variation_total_ips_origem',\n",
    "           file+'_kurt_total_ips_destino',\n",
    "           file+'_skw_total_ips_destino',\n",
    "           file+'_coefficient_variation_total_ips_destino',\n",
    "           file+'_kurt_total_pacotes',\n",
    "           file+'_skw_total_pacotes',\n",
    "           file+'_coefficient_variation_total_pacotes',]\n",
    "\n",
    "df = early_warning[colunas]\n",
    "df = df.fillna(0)\n",
    "label = pd.read_csv('ctulabel.csv')['maior_2']\n",
    "\n",
    "init = 880\n",
    "end = 5632\n",
    "\n",
    "x_test = df[init:end]\n",
    "y_real = label[init:end]\n",
    "tamanho = 30\n",
    "preds = []\n",
    "limiar = 50\n",
    "\n",
    "for i in range(0, 159):\n",
    "    comeco = i*tamanho\n",
    "    final = (i+1)*tamanho\n",
    "    if(final > len(y_real)):\n",
    "        tamanho = len(y_real) - (final - tamanho) \n",
    "        final = len(y_real)\n",
    "        \n",
    "        \n",
    "    array_kernel = ['linear' , 'poly' , 'rbf' ,  ] # 'precomputed','sigmoid'\n",
    "    result = []\n",
    "\n",
    "    #x = FastICA(n_components=9, random_state=0).fit_transform(x_train)\n",
    "    x = x_test[comeco:final]\n",
    "    \n",
    "    nu_atual = 0.05\n",
    "    for c in array_kernel:\n",
    "        while nu_atual <= 0.15:\n",
    "            clf = OneClassSVM(gamma='auto', kernel=c, nu=nu_atual)\n",
    "            y_test = clf.fit_predict(x)\n",
    "            y_test_final = [0 if i == 1 else 1 for i in y_test]\n",
    "            result.append(y_test_final)       \n",
    "            nu_atual += 0.01\n",
    "        nu_atual = 0.05\n",
    "        \n",
    "        \n",
    "    learning_rate = ['constant','optimal', 'invscaling' ,'adaptive']\n",
    "\n",
    "    nu_atual = 0.05\n",
    "    for c in learning_rate:\n",
    "        while nu_atual <= 0.15:\n",
    "            clf = SGDOneClassSVM(learning_rate = c, nu = nu_atual, eta0= 0.1)\n",
    "            y_test = clf.fit_predict(x)\n",
    "            y_test_final = [0 if i == 1 else 1 for i in y_test]\n",
    "            result.append(y_test_final)       \n",
    "            nu_atual += 0.01\n",
    "        nu_atual = 0.05\n",
    "\n",
    "    array_algorithm = ['auto', 'ball_tree', 'kd_tree','brute']\n",
    "    \n",
    "    nu_atual = 0.05\n",
    "    for c in array_algorithm:\n",
    "        while nu_atual <= 0.15:\n",
    "            clf = LocalOutlierFactor(n_neighbors=2, algorithm = c)\n",
    "            y_test = clf.fit_predict(x)\n",
    "            y_test_final = [0 if i == 1 else 1 for i in y_test]\n",
    "            result.append(y_test_final)       \n",
    "            nu_atual += 0.01\n",
    "        nu_atual = 0.05\n",
    "        \n",
    "    cols = [i for i in range(0, tamanho)]\n",
    "    df = pd.DataFrame(result, columns=[cols])\n",
    "    \n",
    "    for col in cols:\n",
    "        preds.append(1 if (df[col].sum()[col] > limiar) else 0)\n",
    "\n",
    "print(preds)\n",
    "print(confusion_matrix(y_real, preds))\n",
    "print(classification_report(y_real, preds, digits = 4)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b436719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4667   33]\n",
      " [  51    1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9892    0.9930    0.9911      4700\n",
      "           1     0.0294    0.0192    0.0233        52\n",
      "\n",
      "    accuracy                         0.9823      4752\n",
      "   macro avg     0.5093    0.5061    0.5072      4752\n",
      "weighted avg     0.9787    0.9823    0.9805      4752\n",
      "\n"
     ]
    }
   ],
   "source": [
    "colunas = early_warning.columns[361:]\n",
    "\n",
    "df = early_warning[colunas]\n",
    "df = df.fillna(0)\n",
    "label = pd.read_csv('ctulabel.csv')['maior_2']\n",
    "\n",
    "init = 880\n",
    "end = 5632\n",
    "\n",
    "x_test = df[init:end]\n",
    "y_real = label[init:end]\n",
    "tamanho = 30\n",
    "preds = []\n",
    "limiar = 70\n",
    "\n",
    "for i in range(0, 159):\n",
    "    comeco = i*tamanho\n",
    "    final = (i+1)*tamanho\n",
    "    if(final > len(y_real)):\n",
    "        tamanho = len(y_real) - (final - tamanho) \n",
    "        final = len(y_real)\n",
    "        \n",
    "        \n",
    "    array_kernel = ['linear' , 'poly' , 'rbf' ,  ] # 'precomputed','sigmoid'\n",
    "    result = []\n",
    "\n",
    "    x = FactorAnalysis(n_components=1, random_state=0).fit_transform(x_test[comeco:final])\n",
    "    #x = PCA(n_components=9, random_state=0).fit_transform(x_test[comeco:final])\n",
    "    # x = x_test[comeco:final]\n",
    "    \n",
    "    nu_atual = 0.05\n",
    "    for c in array_kernel:\n",
    "        while nu_atual <= 0.15:\n",
    "            clf = OneClassSVM(gamma='auto', kernel=c, nu=nu_atual)\n",
    "            y_test = clf.fit_predict(x)\n",
    "            y_test_final = [0 if i == 1 else 1 for i in y_test]\n",
    "            result.append(y_test_final)       \n",
    "            nu_atual += 0.01\n",
    "        nu_atual = 0.05\n",
    "        \n",
    "        \n",
    "    learning_rate = ['constant','optimal', 'invscaling' ,'adaptive']\n",
    "\n",
    "    nu_atual = 0.05\n",
    "    for c in learning_rate:\n",
    "        while nu_atual <= 0.15:\n",
    "            clf = SGDOneClassSVM(learning_rate = c, nu = nu_atual, eta0= 0.1)\n",
    "            y_test = clf.fit_predict(x)\n",
    "            y_test_final = [0 if i == 1 else 1 for i in y_test]\n",
    "            result.append(y_test_final)       \n",
    "            nu_atual += 0.01\n",
    "        nu_atual = 0.05\n",
    "\n",
    "    array_algorithm = ['auto', 'ball_tree', 'kd_tree','brute']\n",
    "    \n",
    "    nu_atual = 0.05\n",
    "    for c in array_algorithm:\n",
    "        while nu_atual <= 0.15:\n",
    "            clf = LocalOutlierFactor(n_neighbors=2, algorithm = c)\n",
    "            y_test = clf.fit_predict(x)\n",
    "            y_test_final = [0 if i == 1 else 1 for i in y_test]\n",
    "            result.append(y_test_final)       \n",
    "            nu_atual += 0.01\n",
    "        nu_atual = 0.05\n",
    "        \n",
    "    cols = [i for i in range(0, tamanho)]\n",
    "    df = pd.DataFrame(result, columns=[cols])\n",
    "    \n",
    "    for col in cols:\n",
    "        preds.append(1 if (df[col].sum()[col] > limiar) else 0)\n",
    "\n",
    "#print(preds)\n",
    "print(confusion_matrix(y_real, preds))\n",
    "print(classification_report(y_real, preds, digits = 4)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b58c6b5-2541-4898-96f8-6a6269ec44f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teste com  1  features\n",
      "[[4664   36]\n",
      " [  51    1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9892    0.9923    0.9908      4700\n",
      "           1     0.0270    0.0192    0.0225        52\n",
      "\n",
      "    accuracy                         0.9817      4752\n",
      "   macro avg     0.5081    0.5058    0.5066      4752\n",
      "weighted avg     0.9787    0.9817    0.9802      4752\n",
      "\n",
      "teste com  2  features\n",
      "[[4691    9]\n",
      " [  52    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9890    0.9981    0.9935      4700\n",
      "           1     0.0000    0.0000    0.0000        52\n",
      "\n",
      "    accuracy                         0.9872      4752\n",
      "   macro avg     0.4945    0.4990    0.4968      4752\n",
      "weighted avg     0.9782    0.9872    0.9827      4752\n",
      "\n",
      "teste com  3  features\n",
      "[[4694    6]\n",
      " [  52    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9890    0.9987    0.9939      4700\n",
      "           1     0.0000    0.0000    0.0000        52\n",
      "\n",
      "    accuracy                         0.9878      4752\n",
      "   macro avg     0.4945    0.4994    0.4969      4752\n",
      "weighted avg     0.9782    0.9878    0.9830      4752\n",
      "\n",
      "teste com  4  features\n",
      "[[4700    0]\n",
      " [  52    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9891    1.0000    0.9945      4700\n",
      "           1     0.0000    0.0000    0.0000        52\n",
      "\n",
      "    accuracy                         0.9891      4752\n",
      "   macro avg     0.4945    0.5000    0.4972      4752\n",
      "weighted avg     0.9782    0.9891    0.9836      4752\n",
      "\n",
      "teste com  5  features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anderson-note/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anderson-note/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anderson-note/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4699    1]\n",
      " [  52    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9891    0.9998    0.9944      4700\n",
      "           1     0.0000    0.0000    0.0000        52\n",
      "\n",
      "    accuracy                         0.9888      4752\n",
      "   macro avg     0.4945    0.4999    0.4972      4752\n",
      "weighted avg     0.9782    0.9888    0.9835      4752\n",
      "\n",
      "teste com  6  features\n",
      "[[4700    0]\n",
      " [  52    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9891    1.0000    0.9945      4700\n",
      "           1     0.0000    0.0000    0.0000        52\n",
      "\n",
      "    accuracy                         0.9891      4752\n",
      "   macro avg     0.4945    0.5000    0.4972      4752\n",
      "weighted avg     0.9782    0.9891    0.9836      4752\n",
      "\n",
      "teste com  7  features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anderson-note/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anderson-note/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anderson-note/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4699    1]\n",
      " [  52    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9891    0.9998    0.9944      4700\n",
      "           1     0.0000    0.0000    0.0000        52\n",
      "\n",
      "    accuracy                         0.9888      4752\n",
      "   macro avg     0.4945    0.4999    0.4972      4752\n",
      "weighted avg     0.9782    0.9888    0.9835      4752\n",
      "\n",
      "teste com  8  features\n",
      "[[4699    1]\n",
      " [  52    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9891    0.9998    0.9944      4700\n",
      "           1     0.0000    0.0000    0.0000        52\n",
      "\n",
      "    accuracy                         0.9888      4752\n",
      "   macro avg     0.4945    0.4999    0.4972      4752\n",
      "weighted avg     0.9782    0.9888    0.9835      4752\n",
      "\n",
      "teste com  9  features\n",
      "[[4700    0]\n",
      " [  52    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9891    1.0000    0.9945      4700\n",
      "           1     0.0000    0.0000    0.0000        52\n",
      "\n",
      "    accuracy                         0.9891      4752\n",
      "   macro avg     0.4945    0.5000    0.4972      4752\n",
      "weighted avg     0.9782    0.9891    0.9836      4752\n",
      "\n",
      "teste com  10  features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anderson-note/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anderson-note/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/anderson-note/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4699    1]\n",
      " [  52    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9891    0.9998    0.9944      4700\n",
      "           1     0.0000    0.0000    0.0000        52\n",
      "\n",
      "    accuracy                         0.9888      4752\n",
      "   macro avg     0.4945    0.4999    0.4972      4752\n",
      "weighted avg     0.9782    0.9888    0.9835      4752\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for feature in range(1,11):\n",
    "    print(\"teste com \", feature, ' features')\n",
    "    colunas = early_warning.columns[361:]\n",
    "\n",
    "    df = early_warning[colunas]\n",
    "    df = df.fillna(0)\n",
    "    label = pd.read_csv('ctulabel.csv')['maior_2']\n",
    "\n",
    "    init = 880\n",
    "    end = 5632\n",
    "\n",
    "    x_test = df[init:end]\n",
    "    y_real = label[init:end]\n",
    "    tamanho = 30\n",
    "    preds = []\n",
    "\n",
    "    qtde_models = 0.35\n",
    "    for i in range(0, 159):\n",
    "        comeco = i*tamanho\n",
    "        final = (i+1)*tamanho\n",
    "        if(final > len(y_real)):\n",
    "            tamanho = len(y_real) - (final - tamanho) \n",
    "            final = len(y_real)\n",
    "\n",
    "        models = 0    \n",
    "        array_kernel = ['linear' , 'poly' , 'rbf' ,'sigmoid'  ] # 'precomputed'\n",
    "        result = []\n",
    "\n",
    "        x = FactorAnalysis(n_components=feature, random_state=0).fit_transform(x_test[comeco:final])\n",
    "        #x = FastICA(n_components=6, random_state=0).fit_transform(x_test[comeco:final])\n",
    "        # x = x_test[comeco:final]\n",
    "\n",
    "        nu_atual = 0.05\n",
    "        for c in array_kernel:\n",
    "            while nu_atual <= qtde_models:\n",
    "                clf = OneClassSVM(gamma='auto', kernel=c, nu=nu_atual)\n",
    "                y_test = clf.fit_predict(x)\n",
    "                y_test_final = [0 if i == 1 else 1 for i in y_test]\n",
    "                result.append(y_test_final)\n",
    "                models = models + 1\n",
    "                nu_atual += 0.01\n",
    "            nu_atual = 0.05\n",
    "\n",
    "\n",
    "        learning_rate = ['constant','optimal', 'invscaling' ,'adaptive']\n",
    "\n",
    "        nu_atual = 0.05\n",
    "        for c in learning_rate:\n",
    "            while nu_atual <= qtde_models:\n",
    "                clf = SGDOneClassSVM(learning_rate = c, nu = nu_atual, eta0= 0.1, random_state=0)\n",
    "                y_test = clf.fit_predict(x)\n",
    "                y_test_final = [0 if i == 1 else 1 for i in y_test]\n",
    "                result.append(y_test_final)  \n",
    "                models = models + 1\n",
    "                nu_atual += 0.01\n",
    "            nu_atual = 0.05\n",
    "\n",
    "        array_algorithm = ['auto', 'ball_tree', 'kd_tree','brute']\n",
    "\n",
    "        # for c in array_algorithm:\n",
    "        #     y_test = LocalOutlierFactor(n_neighbors=2, algorithm = c).fit_predict(x)\n",
    "        #     y_test_final = [0 if i == 1 else 1 for i in y_test]\n",
    "        #     result.append(y_test_final)       \n",
    "        #     models = models + 1\n",
    "\n",
    "\n",
    "        cols = [i for i in range(0, tamanho)]\n",
    "        df = pd.DataFrame(result, columns=[cols])\n",
    "\n",
    "        limiar = models*0.5\n",
    "        #print('limiarrrrrrrrrr', limiar, models)\n",
    "\n",
    "        for col in cols:\n",
    "            preds.append(1 if (df[col].sum()[col] > limiar) else 0)\n",
    "\n",
    "    #print(preds)\n",
    "    print(confusion_matrix(y_real, preds))\n",
    "    print(classification_report(y_real, preds, digits = 4)) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
